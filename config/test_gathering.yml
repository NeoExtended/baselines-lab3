algorithm:
  name: "ppo"
  verbose: 10
  policy:
    name: ActorCriticCustomPolicy
    activation_fn: torch.nn.SELU
    normalize_images: true
    features_extractor_kwargs:
      activation: torch.nn.SELU
#    features_extractor_kwargs:
#      downsample_images: [84, 84]
#    features_extractor_class: policies.CNNExtractor
#    features_extractor_kwargs:
#      arch:
#        - ["conv", 32, 8, 4]
#        - ["pool", 2, 2]
#        - ["conv", 64, 4, 2]
#        - ["conv", 64, 3, 1]
  tensorboard_log: true
  clip_range:
    type: "linear"
    initial_value: 0.3
    final_value: 0.05
  ent_coef: 0.00001
  gamma: 0.99
  gae_lambda: 0.95
  learning_rate:
    type: "piecewise"
    endpoints:
      - [1.0, 0.00012]
      - [0.8, 0.00005]
      - [0.0, 0.00001]
  n_steps: 128
  batch_size: 256
  n_epochs: 16

env:
  name: "gym_maze:Maze0318Continuous-v0"
  wrappers:
    - stable_baselines3.common.atari_wrappers.MaxAndSkipEnv:
          skip: 4
    - gym.wrappers.ResizeObservation:
        shape: [84, 84]
  n_envs: 8
  n_particles: 256
#  multichannel_obs: true
  normalize:
    norm_obs: false
    norm_reward: true
    precompute: true
    samples: 10000


meta:
  seed: 82
  n_timesteps: 1000000
  log_dir: "./logs/Labtest"
  save_interval: 200000
